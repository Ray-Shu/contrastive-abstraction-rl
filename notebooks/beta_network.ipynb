{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\")) \n",
    "if project_root not in sys.path: \n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from models.cl_model import mlpCL \n",
    "from models.cmhn import cmhn \n",
    "\n",
    "from data.DatasetCL import DatasetCL \n",
    "from data.Sampler import Sampler \n",
    "from data.TrajectorySet import TrajectorySet\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at /Users/ray/Documents/Research Assistancy UofA 2025/Reproduce Paper/contrastive-abstraction-RL/saved_models/best_model.ckpt, loading...\n"
     ]
    }
   ],
   "source": [
    "# Load cmhn model \n",
    "mhn = cmhn(update_steps=1)\n",
    "\n",
    "# Load trained CL model \n",
    "model_name = \"best_model.ckpt\"\n",
    "pretrained_model_file = os.path.join(project_root+ \"/saved_models\", model_name) \n",
    "\n",
    "if os.path.isfile(pretrained_model_file): \n",
    "    print(f\"Found pretrained model at {pretrained_model_file}, loading...\") \n",
    "    cl_model = mlpCL.load_from_checkpoint(pretrained_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedBetaModel(pl.LightningModule): \n",
    "    def __init__(self, cmhn, beta_max, lr=1e-3, weight_decay=1e-5, masking_ratio=0.3, max_epochs=1000, input_dim=32, h1=128, h2=32, fc_h1 = 64):\n",
    "        super().__init__() \n",
    "        self.save_hyperparameters()\n",
    "        self.cmhn = cmhn \n",
    "\n",
    "        self.beta_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            nn.ReLU(), \n",
    "\n",
    "            nn.Linear(h1, h2), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(h2, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "        self.fc_nn = nn.Sequential( \n",
    "            nn.Linear(input_dim, fc_h1),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(fc_h1, input_dim)\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                lr= self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr / 50)\n",
    "        return ([optimizer], [lr_scheduler])\n",
    "\n",
    "    def loss(self, batch, mode=\"train\"): \n",
    "        \"\"\"\n",
    "        The loss function for the beta network. \n",
    "\n",
    "        Args: \n",
    "            batch: The batch data that the beta network will use (z representations). \n",
    "        \n",
    "        Returns: \n",
    "            loss: The infoNCE loss. \n",
    "        \"\"\"\n",
    "        # get the trial beta \n",
    "        beta = self.beta_net(batch)  \n",
    "\n",
    "        # get abstract representation 'u' using the hopfield network (from z and beta)\n",
    "        U = torch.empty((0, 32))\n",
    "        for i in range(batch.size()[0]): \n",
    "            u_i = self.cmhn.run(batch, batch[i, :], beta[i])\n",
    "            u_i = torch.transpose(u_i, 1, 0)\n",
    "    \n",
    "            U = torch.cat([U, u_i], dim=0)\n",
    "\n",
    "        U2 = self.cmhn.run_batch(batch, batch, beta) \n",
    "\n",
    "\n",
    "        return U, U2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z representation: tensor([[-14.1995,  16.4025,  -2.2677,  16.8622,   3.8314, -24.4883,  -8.4043,\n",
      "          19.3913, -18.0510, -16.5705,   0.3289,   9.5356,   0.9896,   5.2742,\n",
      "          -2.1161, -13.5134, -10.2675,  -8.2921,  13.5510, -21.6915,  13.4183,\n",
      "           4.1641,  20.5247, -16.7965,  20.0339,   7.7674,   8.0739,  14.6877,\n",
      "          22.4745,  -2.0929,  -0.2420,  25.1546],\n",
      "        [ 17.8858, -26.0878,  -6.0880,  -4.0136,  -3.1719,   9.8321,  14.3580,\n",
      "           4.9684,  -8.7627, -18.6895,  27.3732,   6.5467,  -4.1518,  -3.7496,\n",
      "         -15.4645,  -2.3632, -13.1712,   7.4133,   8.6765, -28.7746,  -4.9537,\n",
      "         -22.8542,   9.5407, -17.5092,  -2.9711,  -7.6025,  16.0034,   2.1269,\n",
      "          23.1665,  10.5346,  -2.3052,  20.7228]])\n",
      "beta tensor([[0.2980],\n",
      "        [0.3188]], grad_fn=<SigmoidBackward0>)\n",
      "1 tensor([[6342.5029, 1773.1714],\n",
      "        [1773.1714, 6371.3105]])\n",
      "2 tensor([[1890.1979,  528.4420],\n",
      "        [ 565.3519, 2031.4069]], grad_fn=<MulBackward0>)\n",
      "probs tensor([[1., 0.],\n",
      "        [0., 1.]], grad_fn=<SoftmaxBackward0>)\n",
      "probs size torch.Size([2, 2])\n",
      "tensor([[-14.1995,  16.4025,  -2.2677,  16.8622,   3.8314, -24.4883,  -8.4043,\n",
      "          19.3913, -18.0510, -16.5705,   0.3289,   9.5356,   0.9896,   5.2742,\n",
      "          -2.1161, -13.5134, -10.2675,  -8.2921,  13.5510, -21.6915,  13.4183,\n",
      "           4.1641,  20.5247, -16.7965,  20.0339,   7.7674,   8.0739,  14.6877,\n",
      "          22.4745,  -2.0929,  -0.2420,  25.1546],\n",
      "        [ 17.8858, -26.0878,  -6.0880,  -4.0136,  -3.1719,   9.8321,  14.3580,\n",
      "           4.9684,  -8.7627, -18.6895,  27.3732,   6.5467,  -4.1518,  -3.7496,\n",
      "         -15.4645,  -2.3632, -13.1712,   7.4133,   8.6765, -28.7746,  -4.9537,\n",
      "         -22.8542,   9.5407, -17.5092,  -2.9711,  -7.6025,  16.0034,   2.1269,\n",
      "          23.1665,  10.5346,  -2.3052,  20.7228]], grad_fn=<CatBackward0>)\n",
      "tensor([[-14.1995,  16.4025,  -2.2677,  16.8622,   3.8314, -24.4883,  -8.4043,\n",
      "          19.3913, -18.0510, -16.5705,   0.3289,   9.5356,   0.9896,   5.2742,\n",
      "          -2.1161, -13.5134, -10.2675,  -8.2921,  13.5510, -21.6915,  13.4183,\n",
      "           4.1641,  20.5247, -16.7965,  20.0339,   7.7674,   8.0739,  14.6877,\n",
      "          22.4745,  -2.0929,  -0.2420,  25.1546],\n",
      "        [ 17.8858, -26.0878,  -6.0880,  -4.0136,  -3.1719,   9.8321,  14.3580,\n",
      "           4.9684,  -8.7627, -18.6895,  27.3732,   6.5467,  -4.1518,  -3.7496,\n",
      "         -15.4645,  -2.3632, -13.1712,   7.4133,   8.6765, -28.7746,  -4.9537,\n",
      "         -22.8542,   9.5407, -17.5092,  -2.9711,  -7.6025,  16.0034,   2.1269,\n",
      "          23.1665,  10.5346,  -2.3052,  20.7228]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.as_tensor([[1,-1, 1, 0.4], [1,2,-0.1, 0.2]])\n",
    "with torch.no_grad():\n",
    "    z = cl_model(x)\n",
    "print(\"z representation:\", z)\n",
    "#print(\"z size:\", z.size())\n",
    "\n",
    "bm = LearnedBetaModel(cmhn=mhn, beta_max=200)\n",
    "U1, U2 = bm.loss(z)\n",
    "\n",
    "print(U1)\n",
    "print(U2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CL_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
