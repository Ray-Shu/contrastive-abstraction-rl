{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set path to parent dir to import personal imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Personal \n",
    "from data.TrajectorySet import TrajectorySet\n",
    "from data.Sampler import Sampler \n",
    "from data.DatasetCL import DatasetCL \n",
    "from utils.tensor_utils import convert_batch_to_tensor\n",
    "\n",
    "# Misc\n",
    "import minari \n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Torch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data \n",
    "\n",
    "# PyTorch Lightning \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minari_dataset = minari.load_dataset(\"D4RL/pointmaze/large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mray-s\u001b[0m (\u001b[33mray-s-university-of-alberta\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpCL(pl.LightningModule): \n",
    "    def __init__(self, lr, weight_decay, temperature=30, max_epochs=1000, h1=256, h2=128, h3=64, h4=32):\n",
    "        super().__init__() # inherit from LightningModule and nn.module \n",
    "        self.save_hyperparameters() # save args  \n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, h1), \n",
    "            nn.ReLU(inplace=True), \n",
    "\n",
    "            nn.Linear(h1, h2), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h2, h3), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h3, h4), # representation z \n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                lr= self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr / 50)\n",
    "        return ([optimizer], [lr_scheduler])\n",
    "    \n",
    "    def info_nce_loss(self, batch, mode=\"train\"): \n",
    "        # Organizes the states such that their positive pairs are (i + N // 2) away. \n",
    "        batch = torch.cat(batch, dim=0)  \n",
    "\n",
    "        # Encode states \n",
    "        z = self.mlp(batch)\n",
    "\n",
    "        # Get cosine similarity matrix, where the i'th row and j'th index correspond to the \n",
    "        # similarity between z_i and z_j \n",
    "        cos_sim = F.cosine_similarity(x1=z[:, None, :], x2=z[None, :, :], dim=-1)\n",
    "\n",
    "        # Create a boolean mask where the diagonals are true\n",
    "        self_mask = torch.eye(n=cos_sim.size()[0], dtype=bool, device=cos_sim.device)\n",
    "\n",
    "        # Change the diagonals to become really small numbers, zero-ing out their similarity value\n",
    "        # i.e we dont want the similarity values of z_i and z_i to be in the calculations \n",
    "        cos_sim = cos_sim.masked_fill(mask=self_mask, value= -9e15)  \n",
    "\n",
    "        # Create a mask that corresponds to the [i,j] location of positive pairs. \n",
    "        # Since positive pairs are i + N // 2 away from the i'th anchor, we roll the cos_sim matrix as such \n",
    "        # Rolling the Identity matrix row-wise will create this effect. \n",
    "        pos_mask = self_mask.roll(shifts=len(batch) // 2, dims=0)\n",
    "\n",
    "        cos_sim = cos_sim / self.hparams.temperature \n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=0)\n",
    "        nll = torch.mean(nll) # Loss \n",
    "\n",
    "\n",
    "        reshaped_cos_sim = cos_sim[pos_mask][:, None] # Create 2D matrix with positive pairs on the first column \n",
    "        comb_sim = torch.cat([reshaped_cos_sim, cos_sim.masked_fill(mask=pos_mask, value=-9e15)], dim=1) # concatenate all other values column wise \n",
    "\n",
    "        # Sort to find if the true positive pair has the highest similarity value with its column neighbours\n",
    "        sim_argsort = comb_sim.argsort(dim=1, descending=True).argmin(dim=1)\n",
    "\n",
    "        # Metrics \n",
    "        correct = (sim_argsort == 0).float().mean() # Average true positive pairs that had the highest similarity in their column neighbours \n",
    "        top5 = (sim_argsort < 5).float().mean() # Average true positive pairs that were in the top5 highest similarity values \n",
    "        mean_position = 1 + sim_argsort.float().mean() # Average position of true positive pairs (using 1-indexing)\n",
    "\n",
    "        # Logging metrics \n",
    "        self.log(f\"{mode}/top1\", correct, prog_bar=True, on_epoch=True)\n",
    "        self.log(f\"{mode}/top5\", top5, prog_bar=True, on_epoch=True)\n",
    "        self.log(f\"{mode}/mean_pos\", mean_position, on_epoch=True)\n",
    "        self.log(f\"{mode}/nll_loss\", nll, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        return nll\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.3912,  2.8372, -3.4308, -0.4404],\n",
      "        [-1.5635,  0.8735,  1.4885, -1.2872],\n",
      "        [ 4.4222,  0.3994,  0.0472,  5.0314],\n",
      "        [-0.5007,  0.8776,  3.9664,  0.4011]]), tensor([[ 1.2745e+00,  2.8308e+00, -4.1192e+00, -4.8980e-02],\n",
      "        [-1.4805e+00,  2.7749e+00, -3.1463e-03, -4.8342e+00],\n",
      "        [ 4.4155e+00,  6.2710e-01, -2.3755e-01,  4.0578e+00],\n",
      "        [ 3.3278e-02,  9.2593e-01,  4.6660e+00,  3.8236e-01]]))\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m mlpCL(lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m---> 15\u001b[0m nll\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo_nce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(nll)\n",
      "Cell \u001b[0;32mIn[12], line 67\u001b[0m, in \u001b[0;36mmlpCL.info_nce_loss\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m     64\u001b[0m mean_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m sim_argsort\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# Average position of true positive pairs (using 1-indexing)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Logging metrics \u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/nll_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/top1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/top5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/mean_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nll\n",
      "File \u001b[0;32m~/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TESTING CELL! \n",
    "\n",
    "Testing InfoNCE loss \n",
    "\"\"\"\n",
    "\n",
    "T = TrajectorySet(dataset=minari_dataset) \n",
    "S = Sampler(T, dist=\"l\")\n",
    "\n",
    "ds = DatasetCL(S, batch_size=4, k=2)\n",
    "batch = ds.get_batch()\n",
    "model = mlpCL(lr = 1, weight_decay=1)\n",
    "print(batch)\n",
    "\n",
    "nll= model.info_nce_loss(batch)\n",
    "\n",
    "print(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DS \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39m\u001b[43mds\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(DS)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "DS = data.DataLoader(dataset=ds)\n",
    "print(next(iter(DS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../saved_models\"\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=CHECKPOINT_PATH,\n",
    "                                      filename=\"best_model\", \n",
    "                                      save_top_k=3, \n",
    "                                      save_weights_only=True, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val/top5\")\n",
    "\n",
    "def train_cl(train_ds, val_ds, logger, max_epochs=1000, **kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=CHECKPOINT_PATH, \n",
    "        logger = logger,\n",
    "        accelerator= \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "        devices=1, \n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   LearningRateMonitor(\"epoch\")]) # creates a model checkpoint when a new max in val/top5 has been reached \n",
    "    train_loader = data.DataLoader(dataset=train_ds, shuffle=True, drop_last=True)\n",
    "    val_loader = data.DataLoader(dataset= val_ds, shuffle=False, drop_last=True)\n",
    "\n",
    "    print(\"train loader:\", len(train_loader))\n",
    "    print(\"val loader:\", len(val_loader))\n",
    "\n",
    "    pl.seed_everything(10)\n",
    "    model = mlpCL(max_epochs=max_epochs, **kwargs) \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    print(\"Best model path:\", checkpoint_callback.best_model_path)\n",
    "    model = mlpCL.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "train loader: 256\n",
      "val loader: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mray-s\u001b[0m (\u001b[33mray-s-university-of-alberta\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/wandb/run-20250617_152723-f6hkr5qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/f6hkr5qf' target=\"_blank\">test-run</a></strong> to <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL' target=\"_blank\">https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/f6hkr5qf' target=\"_blank\">https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/f6hkr5qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/saved_models exists and is not empty.\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | mlp  | Sequential | 44.5 K | train\n",
      "--------------------------------------------\n",
      "44.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "44.5 K    Total params\n",
      "0.178     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 256/256 [00:05<00:00, 44.96it/s, v_num=r5qf, train/top1_step=1.000, train/top5_step=1.000, train/nll_loss_step=0.000, val/top1=1.000, val/top5=1.000, val/nll_loss=0.000, train/top1_epoch=1.000, train/top5_epoch=1.000, train/nll_loss_epoch=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 256/256 [00:05<00:00, 44.94it/s, v_num=r5qf, train/top1_step=1.000, train/top5_step=1.000, train/nll_loss_step=0.000, val/top1=1.000, val/top5=1.000, val/nll_loss=0.000, train/top1_epoch=1.000, train/top5_epoch=1.000, train/nll_loss_epoch=0.000]\n",
      "Best model path: /Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/saved_models/best_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"distribution\": \"g\",\n",
    "    \"batch_size\": 256,\n",
    "    \"k\": 2,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 1e-4, \n",
    "    \"temperature\": 0.08,\n",
    "    \"max_epochs\": 20\n",
    "}\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"Contrastive Learning RL\", \n",
    "    name=\"test-run\", \n",
    "    save_dir = project_root, \n",
    "    log_model=True\n",
    ") \n",
    "\n",
    "dist = config[\"distribution\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "k = config[\"k\"]\n",
    "lr = config[\"lr\"]\n",
    "weight_decay = config[\"weight_decay\"]\n",
    "temperature = config[\"temperature\"]\n",
    "max_epochs = config[\"max_epochs\"]\n",
    "\n",
    "T = TrajectorySet(dataset=minari_dataset)\n",
    "S = Sampler(T, dist=dist)\n",
    "train_dataset = DatasetCL(S, batch_size=batch_size, k=k)\n",
    "\n",
    "val_dataset = DatasetCL(S, batch_size=batch_size, k=k)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "trainer = train_cl(train_ds=train_dataset, \n",
    "                val_ds=val_dataset, \n",
    "                logger=wandb_logger, \n",
    "                max_epochs=max_epochs, \n",
    "                lr=lr, \n",
    "                temperature=temperature, \n",
    "                weight_decay = weight_decay)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
