{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari \n",
    "import numpy as np\n",
    "\n",
    "# Torch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data \n",
    "\n",
    "# PyTorch Lightning \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = minari.load_dataset(\"D4RL/pointmaze/large-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectorySet: \n",
    "    def __init__(self): \n",
    "        \"\"\"\n",
    "        trajectories: a dictionary housing all of the trajectories. The dictionary structure is: \n",
    "            {\n",
    "                1: [trajectory, length of trajectory]\n",
    "                2: [ ... ]\n",
    "                etc...\n",
    "            } \n",
    "\n",
    "        num_trajectories: the number of trajectories currently in the set. \n",
    "        \"\"\"\n",
    "\n",
    "        self.trajectories = {} \n",
    "        self.num_trajectories = 0 \n",
    "    \n",
    "    def add_trajectory(self, trajectory):\n",
    "        self.trajectories[self.num_trajectories] = [trajectory, len(trajectory)] \n",
    "        self.num_trajectories += 1\n",
    "    \n",
    "    def get_num_trajectories(self):\n",
    "        return self.num_trajectories\n",
    "\n",
    "    def get_trajectory(self, index): \n",
    "        assert index < self.num_trajectories, \"Specified index is too large.\"\n",
    "        return self.trajectories[index]\n",
    "    \n",
    "    def get_trajectory_set(self): \n",
    "        return self.trajectories\n",
    "    \n",
    "    def get_total_states(self): \n",
    "        sum = 0\n",
    "        for _, v in self.trajectories.items(): \n",
    "            sum += v[1]\n",
    "        return sum \n",
    "\n",
    "    def generate_trajectories(self, n_trajectories: int = 2): \n",
    "        \"\"\"\n",
    "        Generates a specified number of trajectories and saves them into the TrajectorySet class. \n",
    "\n",
    "        This runs the scripted agent, where the agent uses a PD controller to follow a \n",
    "        path of waypoints generated with QIteration until it reaches the goal.\n",
    "        \"\"\"\n",
    "        ep_data = dataset.sample_episodes(n_episodes=n_trajectories) # sample trajectories\n",
    "\n",
    "        # adds all of the sampled trajectories into the TrajectorySet \n",
    "        for i in range(len(ep_data)):\n",
    "            ep = ep_data[i] \n",
    "\n",
    "            # Note: only saving states since we only need state representations in the encoder \n",
    "            self.add_trajectory(ep.observations[\"observation\"]) \n",
    "\n",
    "\n",
    "###########################################\n",
    "#  Sampler Class\n",
    "###########################################\n",
    "class Sampler(): \n",
    "    def __init__(self, T: TrajectorySet, dist=\"g\"): \n",
    "        \"\"\"\n",
    "        T: The Trajectory Set class \n",
    "        dist: The distribution used for centering over the anchor state. \n",
    "            ['u', 'g', 'l', 'e'] - uniform, gaussian, laplace, exponential\n",
    "        \"\"\"\n",
    "\n",
    "        self.T = T \n",
    "        self.dist = dist\n",
    "\n",
    "    def sample_anchor_state(self, t: list) -> tuple[list, int]: \n",
    "        \"\"\"\n",
    "        Given a trajectory, we sample the anchor state s_i uniformly. \n",
    "\n",
    "        Args: \n",
    "            t: The given trajectory we sample from. \n",
    "\n",
    "        Returns: \n",
    "            A tuple containing [s_i, idx]\n",
    "            s_i: The state that is sampled, represented as a list of (x,y) coordinates and velocities. \n",
    "            idx: The time step of s_i. \n",
    "        \"\"\"\n",
    "        idx = torch.randint(low=0, high=len(t), size=(1,)).item()\n",
    "        s_i = t[idx] \n",
    "        return [s_i, idx]\n",
    "\n",
    "    def sample_positive_pair(self, t: list, anchor_state: tuple[list, int]) -> tuple[list, int]: \n",
    "        \"\"\"\n",
    "        Given the same trajectory that s_i was sampled from, \n",
    "        center a gaussian distribution around s_i to get obtain its positve pair: s_j. \n",
    "        \n",
    "        Args: \n",
    "            t: The given trajectory, which must be the same as the trajectory that was used to sample the anchor state. \n",
    "            anchor_state: The anchor state; a tuple containing [s_i, idx].\n",
    "            s_i: The state itself.\n",
    "            idx: The time step of s_i.\n",
    "            \n",
    "        \n",
    "        Return: \n",
    "            A tuple containing [s_j, idx]\n",
    "            s_j: The state that is sampled, represented as a list of (x,y) coordinates and velocities. \n",
    "            idx: The time step of s_j.    \n",
    "        \"\"\"\n",
    "        std = 15     # we use 15 to replicate the paper's hyperparams \n",
    "        b = 15       # laplace scale hyper param\n",
    "        gamma = 0.99 # exponential hyper param \n",
    "\n",
    "        _, si_idx = anchor_state\n",
    "\n",
    "        while True: \n",
    "            if self.dist == \"u\": \n",
    "                # uniform \n",
    "                sj_idx = torch.randint(low=0, high=len(t), size=(1,))\n",
    "            elif self.dist == \"g\": \n",
    "                # gaussian \n",
    "                sj_idx = torch.normal(mean=si_idx, std=std, size=(1,))\n",
    "            elif self.dist == \"l\": \n",
    "                # laplacian\n",
    "                sj_idx = torch.distributions.laplace.Laplace(loc=si_idx, scale=b).sample() \n",
    "            elif self.dist == \"e\": \n",
    "                # exponential \n",
    "                i = int(torch.distributions.exponential.Exponential(rate=gamma).sample()) + 1   # +1 so we don't get an offset of 0\n",
    "                sj_idx = si_idx + i \n",
    "            else: \n",
    "                # default to gaussian\n",
    "                sj_idx = torch.normal(mean=si_idx, std=std, size=(1,))\n",
    "\n",
    "            sj_idx = int(sj_idx) \n",
    "\n",
    "            # Ensures we don't choose an index out of range or the same state. \n",
    "            if (sj_idx < len(t)) and (sj_idx > 0) and (sj_idx != si_idx): \n",
    "                break \n",
    "        \n",
    "        s_j = t[sj_idx] \n",
    "\n",
    "        return [s_j, sj_idx]\n",
    "    \n",
    "    def sample_batch(self, batch_size=1024, k=2) -> list[tuple]: \n",
    "        \"\"\" \n",
    "        Creates a batch of anchor states, their positive pairs, and negative pairs. \n",
    "        There will be 2(batch_size - 1) amount of negative examples per positive pair.\n",
    "\n",
    "        Args: \n",
    "            T: The trajectory set class (must be empty). \n",
    "            batch_size: The size of the batch to be generated.\n",
    "            k: A hyperparameter that dictates the average number of \n",
    "                positive pairs sampled from the same trajectory. The \n",
    "                lower the number, the lesser the chance of false negatives. \n",
    "        \n",
    "        Returns: \n",
    "            A list of tuples containing the anchor_state and its positive pair. \n",
    "            The list is the same length as batch_size. \n",
    "        \"\"\" \n",
    "\n",
    "        batch = [] \n",
    "\n",
    "        # Generate trajectory set \n",
    "        n_trajectories = batch_size // k\n",
    "        self.T.generate_trajectories(n_trajectories= n_trajectories)\n",
    "\n",
    "        for _ in range(batch_size): \n",
    "            # Sample anchor state \n",
    "            rng = torch.randint(low=0, high=n_trajectories, size=(1,)).item() \n",
    "            t = self.T.get_trajectory(index=rng)[0]\n",
    "            \n",
    "            anchor_state = self.sample_anchor_state(t) \n",
    "\n",
    "            # Sample positive pair \n",
    "            positive_pair = self.sample_positive_pair(t, anchor_state=anchor_state)\n",
    "\n",
    "            # Retrieve states; time-steps aren't necessary. \n",
    "            s_i = anchor_state[0]\n",
    "            s_j = positive_pair[0]\n",
    "\n",
    "            batch.append([s_i, s_j]) \n",
    "\n",
    "        return batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch_to_tensor(batch: list[list, list]) -> tuple[torch.tensor, torch.tensor]: \n",
    "    \"\"\"\n",
    "    Converts the batch to a tuple of tensors. \n",
    "    The first tensor corresponds to the anchor states.\n",
    "    The second tensor corresponds to their corresponding positive pair. \n",
    "    i.e. i'th anchor state in the first tensor will have its positive pair be in the i'th state in the second tensor. \n",
    "    \"\"\"\n",
    "\n",
    "    #unzips the batch into two tuples\n",
    "    a, b = zip(*batch)  \n",
    "\n",
    "    # stack arrays row-wise and then convert to tensor of dtype float (to be compatible w/ model weights)\n",
    "    a_t = torch.tensor(np.stack(a, axis=0), dtype= torch.float32)\n",
    "    b_t = torch.tensor(np.stack(b, axis=0), dtype= torch.float32)\n",
    "\n",
    "    return (a_t, b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpCL(pl.LightningModule): \n",
    "    def __init__(self, lr, weight_decay, temperature=30, max_epochs=1000, h1=256, h2=128, h3=64, h4=32):\n",
    "        super().__init__() # inherit from LightningModule and nn.module \n",
    "        self.save_hyperparameters() # save args  \n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, h1), \n",
    "            nn.ReLU(inplace=True), \n",
    "\n",
    "            nn.Linear(h1, h2), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h2, h3), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h3, h4), # representation z \n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                lr= self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr / 50)\n",
    "        return [optimizer, lr_scheduler]\n",
    "    \n",
    "    def info_nce_loss(self, batch, mode=\"train\"): \n",
    "        # Organizes the states such that their positive pairs are (i + N // 2) away. \n",
    "        batch = convert_batch_to_tensor(batch=batch) \n",
    "        batch = torch.cat(batch, dim=0)  \n",
    "        print(batch)\n",
    "\n",
    "        z = self.mlp(batch)\n",
    "        print(z)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    TESTING CLASS: DELETE LATER!\n",
    "    \"\"\"\n",
    "    def print_hparam(self): \n",
    "        print(\"Hyper parameters:\", self.hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3521,  0.7024,  0.5414, -4.5867],\n",
      "        [ 1.9719, -1.0481, -5.1330,  0.5165],\n",
      "        [ 1.1335, -1.0809, -5.2263, -1.0864],\n",
      "        [ 1.6779, -1.0082, -3.4937,  0.5231],\n",
      "        [ 4.4615,  1.8705, -1.0043, -4.2230],\n",
      "        [ 2.1797, -1.0738, -5.2263,  0.7809],\n",
      "        [ 1.2844, -1.0493, -4.5935, -0.9271],\n",
      "        [ 0.9286, -1.1135, -4.8800, -0.5182]])\n",
      "tensor([[-1.7256e-01, -8.9389e-03,  2.8800e-02, -1.1430e-01, -1.9647e-01,\n",
      "          2.2673e-02, -7.6746e-03,  2.1138e-01, -2.7661e-01, -6.4846e-02,\n",
      "          1.2426e-01,  3.5447e-02, -5.9387e-03, -2.4825e-01, -3.5725e-01,\n",
      "          4.6524e-02,  1.3229e-01,  5.0340e-02, -6.4042e-02,  1.4493e-01,\n",
      "          5.5383e-02, -3.8025e-02, -8.6073e-02,  7.0000e-02,  8.3061e-02,\n",
      "         -4.4152e-02,  1.5138e-02,  2.8637e-02,  1.1652e-01,  7.3220e-02,\n",
      "          1.2160e-01,  5.5691e-02],\n",
      "        [-1.0891e-01,  2.7514e-02,  7.7591e-02, -1.4019e-01, -1.0129e-01,\n",
      "          5.5456e-03, -2.2768e-02,  1.7631e-01, -1.6983e-01, -4.5114e-02,\n",
      "          1.1631e-01, -4.0896e-02,  3.4529e-03, -1.5925e-01, -1.7619e-01,\n",
      "         -1.9228e-02,  4.4416e-02,  3.5515e-02, -4.6934e-03,  1.6156e-01,\n",
      "          8.6479e-02,  1.1624e-01, -1.0354e-01,  6.9084e-02,  1.1640e-01,\n",
      "         -1.1572e-01,  4.3140e-02,  9.1635e-02,  1.1965e-01,  1.2533e-01,\n",
      "          2.7729e-02,  5.5190e-02],\n",
      "        [-1.0713e-01,  1.3327e-02,  5.2373e-02, -1.3046e-01, -1.5666e-01,\n",
      "         -4.0033e-02, -1.0447e-02,  1.6196e-01, -1.9106e-01, -7.9130e-02,\n",
      "          1.3200e-01, -9.3091e-03, -1.1449e-02, -1.8096e-01, -2.2462e-01,\n",
      "         -1.1358e-02,  9.1175e-02,  4.9346e-02, -3.5545e-02,  1.5044e-01,\n",
      "          7.8712e-02,  9.1398e-02, -9.9359e-02,  9.0313e-02,  1.1173e-01,\n",
      "         -8.4845e-02,  5.1069e-02,  1.3741e-01,  1.5523e-01,  1.4464e-01,\n",
      "          2.6435e-02,  7.5802e-02],\n",
      "        [-1.1595e-01,  5.2242e-02,  7.6239e-02, -1.4437e-01, -7.9177e-02,\n",
      "         -3.0213e-02, -3.2748e-02,  1.3452e-01, -1.4187e-01, -3.6162e-02,\n",
      "          1.1434e-01, -4.1041e-02,  2.5636e-02, -1.2931e-01, -1.5152e-01,\n",
      "         -3.0043e-02,  1.6150e-02, -1.1155e-02, -2.7582e-02,  1.3359e-01,\n",
      "          1.0156e-01,  1.0029e-01, -9.0418e-02,  8.1227e-02,  9.4213e-02,\n",
      "         -1.0624e-01,  5.6087e-02,  8.7676e-02,  8.8334e-02,  1.0726e-01,\n",
      "          3.7307e-02,  7.0807e-02],\n",
      "        [-1.3155e-01,  5.0169e-03,  7.8072e-03, -8.8401e-02, -1.4718e-01,\n",
      "          4.0712e-02, -3.0521e-02,  2.1100e-01, -2.2227e-01, -1.1196e-01,\n",
      "          1.1153e-01,  4.9354e-02, -4.7947e-02, -2.0492e-01, -3.0266e-01,\n",
      "          9.6508e-03,  8.4939e-02,  6.4347e-02, -2.2893e-02,  1.4448e-01,\n",
      "          7.3240e-02, -6.5103e-03, -8.4692e-02,  6.8287e-02,  1.0412e-01,\n",
      "         -3.5118e-02, -7.3304e-02,  3.4444e-02,  7.6132e-02,  2.2691e-02,\n",
      "          1.2739e-01,  4.2640e-02],\n",
      "        [-1.1082e-01,  3.2471e-02,  7.9093e-02, -1.4680e-01, -8.9555e-02,\n",
      "          1.2332e-02, -2.7927e-02,  1.8268e-01, -1.6544e-01, -3.8097e-02,\n",
      "          1.1707e-01, -4.1789e-02,  4.0980e-03, -1.5731e-01, -1.7039e-01,\n",
      "         -2.1194e-02,  4.3815e-02,  3.8932e-02,  1.3203e-03,  1.6060e-01,\n",
      "          8.8049e-02,  1.2094e-01, -1.0630e-01,  6.5989e-02,  1.1675e-01,\n",
      "         -1.2107e-01,  3.8159e-02,  8.2078e-02,  1.2152e-01,  1.2822e-01,\n",
      "          3.0124e-02,  5.1696e-02],\n",
      "        [-1.0744e-01,  2.2005e-02,  5.6976e-02, -1.2983e-01, -1.4120e-01,\n",
      "         -4.1554e-02, -1.4304e-02,  1.4607e-01, -1.7979e-01, -7.7491e-02,\n",
      "          1.2862e-01, -1.7852e-02, -1.5382e-04, -1.7229e-01, -2.1902e-01,\n",
      "         -1.7489e-02,  7.1536e-02,  3.1613e-02, -3.3684e-02,  1.4501e-01,\n",
      "          8.1541e-02,  8.7205e-02, -9.7769e-02,  9.2331e-02,  1.0165e-01,\n",
      "         -8.9009e-02,  5.7657e-02,  1.3251e-01,  1.4001e-01,  1.3511e-01,\n",
      "          2.3322e-02,  8.3222e-02],\n",
      "        [-1.0617e-01,  2.6659e-02,  6.1561e-02, -1.3079e-01, -1.3869e-01,\n",
      "         -4.4135e-02, -2.3180e-02,  1.5649e-01, -1.7787e-01, -6.9297e-02,\n",
      "          1.2762e-01, -1.5035e-02,  3.5770e-03, -1.7076e-01, -1.9092e-01,\n",
      "         -1.2566e-02,  6.0494e-02,  2.8253e-02, -3.5538e-02,  1.5806e-01,\n",
      "          9.3769e-02,  9.2830e-02, -8.8997e-02,  7.6733e-02,  1.1215e-01,\n",
      "         -8.8849e-02,  5.4215e-02,  1.2654e-01,  1.2658e-01,  1.2588e-01,\n",
      "          3.0773e-02,  7.5695e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = TrajectorySet() \n",
    "S = Sampler(T, dist=\"l\")\n",
    "\n",
    "batch = S.sample_batch(4)\n",
    "\n",
    "model = mlpCL(1, 1)\n",
    "model.info_nce_loss(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
