{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set path to parent dir to import personal imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Personal \n",
    "from data.TrajectorySet import TrajectorySet\n",
    "from data.Sampler import Sampler \n",
    "from data.DatasetCL import DatasetCL \n",
    "from utils.tensor_utils import convert_batch_to_tensor\n",
    "\n",
    "# Misc\n",
    "import minari \n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Torch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data \n",
    "\n",
    "# PyTorch Lightning \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minari_dataset = minari.load_dataset(\"D4RL/pointmaze/large-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpCL(pl.LightningModule): \n",
    "    def __init__(self, lr, weight_decay, temperature=30, max_epochs=1000, h1=256, h2=128, h3=64, h4=32):\n",
    "        super().__init__() # inherit from LightningModule and nn.module \n",
    "        self.save_hyperparameters() # save args  \n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, h1), \n",
    "            nn.ReLU(inplace=True), \n",
    "\n",
    "            nn.Linear(h1, h2), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h2, h3), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(h3, h4), # representation z \n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                lr= self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr / 50)\n",
    "        return ([optimizer], [lr_scheduler])\n",
    "    \n",
    "    \n",
    "    def info_nce_loss(self, batch, mode=\"train\"):\n",
    "        # batch is of shape: [N, D]\n",
    "        x = torch.cat(batch, dim=0)  # shape: [2N, D]\n",
    "\n",
    "        z = F.normalize(self.mlp(x), dim=1)  # [2N, h4]\n",
    "        N = z.size(0) // 2\n",
    "\n",
    "        sim = torch.matmul(z, z.T) / self.hparams.temperature  # cosine sim matrix [2N, 2N]\n",
    "\n",
    "        # mask out self similarities\n",
    "        mask = torch.eye(2 * N, device=sim.device).bool()\n",
    "        sim = sim.masked_fill_(mask, -9e15)\n",
    "\n",
    "        # positives: i-th sample matches i + N mod 2N\n",
    "        pos_idx = (torch.arange(2 * N, device=sim.device) + N) % (2 * N)\n",
    "        labels = pos_idx\n",
    "\n",
    "        loss = F.cross_entropy(sim, labels)\n",
    "\n",
    "        # metrics\n",
    "        preds = sim.argmax(dim=1)\n",
    "        top1 = (preds == labels).float().mean()   # top1: true positive is most similar to anchor \n",
    "        top5 = (sim.topk(5, dim=1).indices == labels.unsqueeze(1)).any(dim=1).float().mean() # top5: true positive is atleast in the top 5 most similar to anchor \n",
    "\n",
    "        self.log(f\"{mode}/nll_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top1\", top1, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top5\", top5, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def info_nce_loss(self, batch, mode=\"train\"):\n",
    "        # batch is of shape: [N, D]\n",
    "        x = torch.cat(batch, dim=0)  # shape: [2N, D]\n",
    "\n",
    "        print(x) \n",
    "\n",
    "        z = F.normalize(self.mlp(x), dim=1)  # [2N, h4]\n",
    "        N = z.size(0) // 2\n",
    "        sim = torch.matmul(z, z.T) / self.hparams.temperature  # cosine sim matrix [2N, 2N]\n",
    "\n",
    "        # mask out self similarities\n",
    "        mask = torch.eye(2 * N, device=sim.device).bool()\n",
    "        sim = sim.masked_fill_(mask, -9e15)\n",
    "\n",
    "        # positives: i-th sample matches i + N mod 2N\n",
    "        pos_idx = (torch.arange(2 * N, device=sim.device) + N) % (2 * N)\n",
    "        labels = pos_idx\n",
    "\n",
    "\n",
    "        loss = F.cross_entropy(sim, labels)\n",
    "\n",
    "        # extra statistics \n",
    "        if mode==\"train\": \n",
    "            with torch.no_grad(): \n",
    "                norms = torch.norm(z, dim=1)\n",
    "                self.log(f\"{mode}/sim_mean\", sim.mean(), on_epoch=True)\n",
    "                self.log(f\"{mode}/sim_std\", sim.std(), on_epoch=True)\n",
    "                self.log(f\"{mode}/z_norm_mean\", norms.mean(), on_epoch=True)\n",
    "                self.log(f\"{mode}/z_norm_std\", norms.std(), on_epoch=True)\n",
    "\n",
    "        # metrics\n",
    "        preds = sim.argmax(dim=1)\n",
    "        top1 = (preds == labels).float().mean()   # top1: true positive is most similar to anchor \n",
    "        top5 = (sim.topk(5, dim=1).indices == labels.unsqueeze(1)).any(dim=1).float().mean() # top5: true positive is atleast in the top 5 most similar to anchor \n",
    "\n",
    "        self.log(f\"{mode}/nll_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top1\", top1, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top5\", top5, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([-1.7115,  1.3644,  0.8578,  4.0238]), tensor([-1.6537,  1.5729,  1.3068,  3.9129])), (tensor([-4.5368, -0.7112,  0.1737, -4.6338]), tensor([-4.6236, -0.0486,  0.3660, -3.4841])), (tensor([ 1.7615, -1.1760, -4.7834, -0.2243]), tensor([ 1.6267, -1.1908, -4.4367, -0.4113])), (tensor([ 0.4385,  0.1298, -0.5565,  4.0402]), tensor([ 0.4600, -0.1992,  0.0629,  3.6413])), (tensor([ 0.3336, -0.8223, -0.7331,  2.8372]), tensor([ 0.3272, -0.7916, -0.6360,  3.0686])), (tensor([0.4313, 2.9266, 1.8052, 1.1417]), tensor([0.3645, 2.8499, 0.6275, 2.1529])), (tensor([ 1.4796,  2.9024,  3.9579, -0.9129]), tensor([0.5268, 2.9509, 2.7374, 0.3727])), (tensor([ 3.7407, -1.1543, -4.7900,  0.4174]), tensor([ 2.9743, -1.0957, -5.1797,  0.1534]))]\n",
      "[tensor([[-1.7115,  1.3644,  0.8578,  4.0238],\n",
      "        [-4.5368, -0.7112,  0.1737, -4.6338],\n",
      "        [ 1.7615, -1.1760, -4.7834, -0.2243],\n",
      "        [ 0.4385,  0.1298, -0.5565,  4.0402],\n",
      "        [ 0.3336, -0.8223, -0.7331,  2.8372],\n",
      "        [ 0.4313,  2.9266,  1.8052,  1.1417],\n",
      "        [ 1.4796,  2.9024,  3.9579, -0.9129],\n",
      "        [ 3.7407, -1.1543, -4.7900,  0.4174]]), tensor([[-1.6537,  1.5729,  1.3068,  3.9129],\n",
      "        [-4.6236, -0.0486,  0.3660, -3.4841],\n",
      "        [ 1.6267, -1.1908, -4.4367, -0.4113],\n",
      "        [ 0.4600, -0.1992,  0.0629,  3.6413],\n",
      "        [ 0.3272, -0.7916, -0.6360,  3.0686],\n",
      "        [ 0.3645,  2.8499,  0.6275,  2.1529],\n",
      "        [ 0.5268,  2.9509,  2.7374,  0.3727],\n",
      "        [ 2.9743, -1.0957, -5.1797,  0.1534]])]\n",
      "sim matrix: tensor([[-9.0000e+15,  2.7707e-02,  2.5799e-02,  3.1515e-02,  3.0589e-02,\n",
      "          2.9968e-02,  2.8651e-02,  2.5117e-02,  3.3240e-02,  2.8320e-02,\n",
      "          2.6166e-02,  3.1292e-02,  3.0699e-02,  3.1076e-02,  2.9643e-02,\n",
      "          2.5027e-02],\n",
      "        [ 2.7707e-02, -9.0000e+15,  2.5935e-02,  2.6569e-02,  2.6223e-02,\n",
      "          2.5316e-02,  2.6513e-02,  2.5427e-02,  2.7853e-02,  3.2965e-02,\n",
      "          2.6055e-02,  2.6077e-02,  2.6257e-02,  2.5415e-02,  2.6678e-02,\n",
      "          2.5406e-02],\n",
      "        [ 2.5799e-02,  2.5935e-02, -9.0000e+15,  2.7384e-02,  2.6311e-02,\n",
      "          2.2556e-02,  2.3888e-02,  3.2710e-02,  2.5346e-02,  2.5390e-02,\n",
      "          3.3297e-02,  2.5679e-02,  2.6165e-02,  2.4595e-02,  2.3517e-02,\n",
      "          3.3128e-02],\n",
      "        [ 3.1515e-02,  2.6569e-02,  2.7384e-02, -9.0000e+15,  3.2085e-02,\n",
      "          2.8797e-02,  2.6452e-02,  2.6315e-02,  3.1375e-02,  2.6661e-02,\n",
      "          2.7778e-02,  3.2883e-02,  3.2286e-02,  3.0148e-02,  2.7988e-02,\n",
      "          2.6551e-02],\n",
      "        [ 3.0589e-02,  2.6223e-02,  2.6311e-02,  3.2085e-02, -9.0000e+15,\n",
      "          2.9423e-02,  2.6284e-02,  2.5026e-02,  3.0656e-02,  2.6590e-02,\n",
      "          2.6898e-02,  3.2441e-02,  3.3298e-02,  2.9679e-02,  2.8633e-02,\n",
      "          2.5169e-02],\n",
      "        [ 2.9968e-02,  2.5316e-02,  2.2556e-02,  2.8797e-02,  2.9423e-02,\n",
      "         -9.0000e+15,  3.0674e-02,  2.1459e-02,  3.0288e-02,  2.6493e-02,\n",
      "          2.2840e-02,  2.9023e-02,  2.9400e-02,  3.2122e-02,  3.2423e-02,\n",
      "          2.1532e-02],\n",
      "        [ 2.8651e-02,  2.6513e-02,  2.3888e-02,  2.6452e-02,  2.6284e-02,\n",
      "          3.0674e-02, -9.0000e+15,  2.2320e-02,  2.8825e-02,  2.6828e-02,\n",
      "          2.3906e-02,  2.5885e-02,  2.6323e-02,  2.9183e-02,  3.2427e-02,\n",
      "          2.2812e-02],\n",
      "        [ 2.5117e-02,  2.5427e-02,  3.2710e-02,  2.6315e-02,  2.5026e-02,\n",
      "          2.1459e-02,  2.2320e-02, -9.0000e+15,  2.4759e-02,  2.5181e-02,\n",
      "          3.2678e-02,  2.4833e-02,  2.4896e-02,  2.4049e-02,  2.1928e-02,\n",
      "          3.3153e-02],\n",
      "        [ 3.3240e-02,  2.7853e-02,  2.5346e-02,  3.1375e-02,  3.0656e-02,\n",
      "          3.0288e-02,  2.8825e-02,  2.4759e-02, -9.0000e+15,  2.8572e-02,\n",
      "          2.5734e-02,  3.1411e-02,  3.0781e-02,  3.1127e-02,  2.9959e-02,\n",
      "          2.4591e-02],\n",
      "        [ 2.8320e-02,  3.2965e-02,  2.5390e-02,  2.6661e-02,  2.6590e-02,\n",
      "          2.6493e-02,  2.6828e-02,  2.5181e-02,  2.8572e-02, -9.0000e+15,\n",
      "          2.5526e-02,  2.6522e-02,  2.6603e-02,  2.6417e-02,  2.7383e-02,\n",
      "          2.4967e-02],\n",
      "        [ 2.6166e-02,  2.6055e-02,  3.3297e-02,  2.7778e-02,  2.6898e-02,\n",
      "          2.2840e-02,  2.3906e-02,  3.2678e-02,  2.5734e-02,  2.5526e-02,\n",
      "         -9.0000e+15,  2.6199e-02,  2.6738e-02,  2.4828e-02,  2.3723e-02,\n",
      "          3.3044e-02],\n",
      "        [ 3.1292e-02,  2.6077e-02,  2.5679e-02,  3.2883e-02,  3.2441e-02,\n",
      "          2.9023e-02,  2.5885e-02,  2.4833e-02,  3.1411e-02,  2.6522e-02,\n",
      "          2.6199e-02, -9.0000e+15,  3.2699e-02,  2.9615e-02,  2.7948e-02,\n",
      "          2.4824e-02],\n",
      "        [ 3.0699e-02,  2.6257e-02,  2.6165e-02,  3.2286e-02,  3.3298e-02,\n",
      "          2.9400e-02,  2.6323e-02,  2.4896e-02,  3.0781e-02,  2.6603e-02,\n",
      "          2.6738e-02,  3.2699e-02, -9.0000e+15,  2.9608e-02,  2.8615e-02,\n",
      "          2.5042e-02],\n",
      "        [ 3.1076e-02,  2.5415e-02,  2.4595e-02,  3.0148e-02,  2.9679e-02,\n",
      "          3.2122e-02,  2.9183e-02,  2.4049e-02,  3.1127e-02,  2.6417e-02,\n",
      "          2.4828e-02,  2.9615e-02,  2.9608e-02, -9.0000e+15,  3.0581e-02,\n",
      "          2.3990e-02],\n",
      "        [ 2.9643e-02,  2.6678e-02,  2.3517e-02,  2.7988e-02,  2.8633e-02,\n",
      "          3.2423e-02,  3.2427e-02,  2.1928e-02,  2.9959e-02,  2.7383e-02,\n",
      "          2.3723e-02,  2.7948e-02,  2.8615e-02,  3.0581e-02, -9.0000e+15,\n",
      "          2.2289e-02],\n",
      "        [ 2.5027e-02,  2.5406e-02,  3.3128e-02,  2.6551e-02,  2.5169e-02,\n",
      "          2.1532e-02,  2.2812e-02,  3.3153e-02,  2.4591e-02,  2.4967e-02,\n",
      "          3.3044e-02,  2.4824e-02,  2.5042e-02,  2.3990e-02,  2.2289e-02,\n",
      "         -9.0000e+15]], grad_fn=<MaskedFillBackward0>)\n",
      "labels: tensor([ 8,  9, 10, 11, 12, 13, 14, 15,  0,  1,  2,  3,  4,  5,  6,  7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_Cuda\\lib\\site-packages\\pytorch_lightning\\core\\module.py:441: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.7028, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TESTING CELL! \n",
    "\n",
    "Testing InfoNCE loss \n",
    "\"\"\"\n",
    "\n",
    "T = TrajectorySet(dataset=minari_dataset) \n",
    "S = Sampler(T, dist=\"l\")\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "ds = DatasetCL(S, batch_size=batch_size, k=2)\n",
    "print(ds.get_batch())\n",
    "train_loader= data.DataLoader(dataset=ds, batch_size=batch_size)\n",
    "model = mlpCL(lr = 1, weight_decay=1)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(batch)\n",
    "model.info_nce_loss2(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../saved_models\"\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=CHECKPOINT_PATH,\n",
    "                                      filename=\"best_model\", \n",
    "                                      save_top_k=3, \n",
    "                                      save_weights_only=True, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val/top5\")\n",
    "\n",
    "def train_cl(train_ds, val_ds, batch_size, logger, max_epochs=1000, **kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=CHECKPOINT_PATH, \n",
    "        logger = logger,\n",
    "        accelerator= \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "        devices=1, \n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   LearningRateMonitor(\"epoch\")]) # creates a model checkpoint when a new max in val/top5 has been reached \n",
    "    train_loader = data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = data.DataLoader(dataset= val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    pl.seed_everything(10)\n",
    "    model = mlpCL(max_epochs=max_epochs, **kwargs) \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    print(\"Best model path:\", checkpoint_callback.best_model_path)\n",
    "    model = mlpCL.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/wandb/run-20250618_150815-fzsull7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/fzsull7m' target=\"_blank\">test-run-new-infoNCE-loss</a></strong> to <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL' target=\"_blank\">https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/fzsull7m' target=\"_blank\">https://wandb.ai/ray-s-university-of-alberta/Contrastive%20Learning%20RL/runs/fzsull7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/saved_models exists and is not empty.\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | mlp  | Sequential | 44.5 K | train\n",
      "--------------------------------------------\n",
      "44.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "44.5 K    Total params\n",
      "0.178     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/CL_RL/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 26.66it/s, v_num=ll7m, train/nll_loss_step=3.570, train/top1_step=0.180, train/top5_step=0.463, val/nll_loss=3.740, val/top1=0.209, val/top5=0.496, train/nll_loss_epoch=3.570, train/top1_epoch=0.180, train/top5_epoch=0.463] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 21.25it/s, v_num=ll7m, train/nll_loss_step=3.570, train/top1_step=0.180, train/top5_step=0.463, val/nll_loss=3.740, val/top1=0.209, val/top5=0.496, train/nll_loss_epoch=3.570, train/top1_epoch=0.180, train/top5_epoch=0.463]\n",
      "Best model path: /Users/ray/Documents/Research Assistancy UofA 2025/Reproduce CL/contrastive-learning-RL/saved_models/best_model-v13.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"distribution\": \"g\",\n",
    "    \"batch_size\": 256,\n",
    "    \"k\": 2,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 1e-4, \n",
    "    \"temperature\": 0.08,\n",
    "    \"max_epochs\": 10\n",
    "}\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"Contrastive Learning RL\", \n",
    "    name=\"test-run-new-infoNCE-loss\", \n",
    "    save_dir = project_root, \n",
    "    log_model=True,\n",
    "    config = config\n",
    ") \n",
    "\n",
    "dist = config[\"distribution\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "k = config[\"k\"]\n",
    "lr = config[\"lr\"]\n",
    "weight_decay = config[\"weight_decay\"]\n",
    "temperature = config[\"temperature\"]\n",
    "max_epochs = config[\"max_epochs\"]\n",
    "\n",
    "T = TrajectorySet(dataset=minari_dataset)\n",
    "S = Sampler(T, dist=dist)\n",
    "train_dataset = DatasetCL(S, batch_size=batch_size, k=k)\n",
    "\n",
    "val_dataset = DatasetCL(S, batch_size=batch_size, k=k)\n",
    "\n",
    "model = train_cl(train_ds=train_dataset, \n",
    "                val_ds=val_dataset, \n",
    "                batch_size=batch_size,\n",
    "                logger=wandb_logger, \n",
    "                max_epochs=max_epochs, \n",
    "                lr=lr, \n",
    "                temperature=temperature, \n",
    "                weight_decay = weight_decay)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CL_RL_Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
