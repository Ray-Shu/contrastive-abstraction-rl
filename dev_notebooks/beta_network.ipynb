{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\Documents\\2025 RA\\contrastive-learning-RL\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\")) \n",
    "if project_root not in sys.path: \n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import minari\n",
    "\n",
    "import faiss \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from src.models.cl_model import mlpCL \n",
    "#from src.models.cmhn import cmhn \n",
    "\n",
    "from src.utils.sampling_states import sample_states\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMADE SOME CHANGES TO CMHN AND BETA MODEL: check chatgpt \"gradient flow analysis\" for most of the changes \\n- removed using cmhn on z to get z\\' \\n- normalized dropout on z, changes the magnitude but doesn\\'t affect direction and thus also the cos_sim calcs\\n- in CMHN, changed the __build_index to use inner product (IP) and normalized the input argument \\n- in CMHN, stabilized the softmax at the end with logits - logits.max\\n\\nTRAIN WITH AN IPYNB FILE (copy stuff from beta_main.ipynb) to see results \\n- in CMHN, changed to use torch.bmm for a more efficient computation instead of my own cos sim computation \\n- removed use_gpu in CMHN args \\n- changed pairing (u,z\\') to (u, u\\') for more representative learning \\n- combined getting u and u\\' by concatenating batch and noisy batch into the cmhn (faster)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MADE SOME CHANGES TO CMHN AND BETA MODEL: check chatgpt \"gradient flow analysis\" for most of the changes \n",
    "- removed using cmhn on z to get z' \n",
    "- normalized dropout on z, changes the magnitude but doesn't affect direction and thus also the cos_sim calcs\n",
    "- in CMHN, changed the __build_index to use inner product (IP) and normalized the input argument \n",
    "- in CMHN, stabilized the softmax at the end with logits - logits.max\n",
    "\n",
    "TRAIN WITH AN IPYNB FILE (copy stuff from beta_main.ipynb) to see results \n",
    "- in CMHN, changed to use torch.bmm for a more efficient computation instead of my own cos sim computation \n",
    "- removed use_gpu in CMHN args \n",
    "- changed pairing (u,z') to (u, u') for more representative learning \n",
    "- combined getting u and u' by concatenating batch and noisy batch into the cmhn (faster)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at c:\\Users\\ray\\Documents\\2025 RA\\contrastive-learning-RL/trained_models\\laplace_cos_sim-v1.ckpt, loading...\n"
     ]
    }
   ],
   "source": [
    "# Import minari dataset\n",
    "MINARI_DATASET = minari.load_dataset(\"D4RL/pointmaze/large-v2\")\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Load trained CL model \n",
    "model_name = \"laplace_cos_sim-v1.ckpt\"\n",
    "pretrained_model_file = os.path.join(project_root+ \"/trained_models\", model_name) \n",
    "\n",
    "if os.path.isfile(pretrained_model_file): \n",
    "    print(f\"Found pretrained model at {pretrained_model_file}, loading...\") \n",
    "    cl_model = mlpCL.load_from_checkpoint(pretrained_model_file, map_location=torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_normalized(v, tol=1e-6):\n",
    "    return (v.norm(dim=-1) - 1).abs() < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cmhn(): \n",
    "    def __init__(self, max_iter = 100, threshold = 0.95, topk = 512, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Continuous Modern Hopfield Network \n",
    "\n",
    "        Args: \n",
    "            update_steps: The number of iterations the cmhn will do. (Usually just one).\n",
    "            topk: Using faiss, only the top k most similar patterns will be used. (more efficient in batch-wise updates) \n",
    "            use_gpu: Tells faiss if we use faiss-cpu or faiss-gpu for behind the scenes calculations. \n",
    "            device: The device that torch will use. \n",
    "        \"\"\"\n",
    "        self.max_iter = max_iter \n",
    "        self.threshold = threshold\n",
    "        self.topk = topk\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.index = None \n",
    "\n",
    "    def build_index(self, X, d): \n",
    "        \"\"\"\n",
    "        Builds a faiss index (an object) for efficient searching of top-k patterns from X (on cpu). \n",
    "        \"\"\"\n",
    "        X_np = X.detach().cpu().numpy().astype(\"float32\") # convert X from tensor to numpy \n",
    "\n",
    "        self.index = faiss.IndexFlatIP(d)\n",
    "        \n",
    "        self.index.add(X_np)\n",
    "    \n",
    "    def __update(self, X, xi, beta): \n",
    "        \"\"\"\n",
    "        The update rule for a continuous modern hopfield network. \n",
    "\n",
    "        Args: \n",
    "            X: The stored patterns. X is of size [N, d], where N is the number of patterns, and d the size of the patterns. \n",
    "            xi: The state pattern (ie. the current pattern being updated). xi is of size [d, 1]. \n",
    "            beta: The scalar inverse-temperature hyperparamater. Controls the number of metastable states that occur in the energy landscape. \n",
    "                - High beta corresponds to low temp, more separation between patterns.  \n",
    "                - Low beta corresponds to high temp, less separation (more metastable states). \n",
    "        \"\"\"\n",
    "        X_norm = F.normalize(X, p=2, dim=1)\n",
    "        xi_norm = F.normalize(xi, p=2, dim=0)\n",
    "        sims = X_norm @ xi_norm  # simularity between stored patterns and current pattern \n",
    "        p = F.softmax(beta * sims, dim=0, dtype=torch.float32)  # softmax dist along patterns (higher probability => more likely to be that stored pattern)\n",
    "        # p of size [N, 1] \n",
    "\n",
    "        X_T = X_norm.transpose(0, 1) \n",
    "        xi_new = X_T @ p  # xi_new, the updated state pattern; size [d, 1]\n",
    "        return xi_new\n",
    "\n",
    "    def __run_batch(self, X, queries, beta=None): \n",
    "        \"\"\"\n",
    "        Runs the mhn batch-wise for efficient computation. \n",
    "\n",
    "        Args: \n",
    "            X: Stored patterns, size [N, d].\n",
    "            queries: Input queries, size [N, d].\n",
    "            beta: The beta value per sample, size [N].\n",
    "        \"\"\"        \n",
    "        \n",
    "        assert beta != None, \"Must have a value for beta.\" \n",
    "\n",
    "        # normalize for cos sim calcs\n",
    "        X_norm = F.normalize(X, p=2, dim=-1)\n",
    "        queries_norm = F.normalize(queries, p=2, dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            queries_np = queries_norm.detach().cpu().numpy().astype(\"float32\")\n",
    "            _, indices = self.index.search(queries_np, self.topk)\n",
    "            indices = torch.from_numpy(indices).to(X.device) # indices of shape [N, topk]\n",
    "\n",
    "        topk_X = X_norm[indices] # size [N, topk, d] \n",
    "        topk_q = queries_norm.unsqueeze(1) # change queries from [N, d] to [N, 1, d] for broadcasting\n",
    "        \n",
    "        # dot product of x_ij * q_i along \"d dim\" to obtain tensor of [N, topk]\n",
    "        # q_i represents the i'th query\n",
    "        # x_ij represents the corresponding i'th query and j'th pattern, where j is among the topk \n",
    "        # then sum over d to obtain the similarity between row i and col j. \n",
    "        # sims = torch.sum(topk_X * topk_q, dim=-1) \n",
    "\n",
    "        # USE torch.bmm instead of the above comments for more efficient computation (they do the same thing tho) \n",
    "        sims = torch.bmm(topk_q, topk_X.transpose(1,2)).squeeze(1)\n",
    "\n",
    "        # removing beta broadcasting\n",
    "        #beta = beta.view(-1, 1)  # beta: [N, 1], broadcasting beta. \n",
    "        logits = beta * sims       # sims * beta: [N, topk]\n",
    "        logits_max = torch.max(logits, dim=-1, keepdim=True)[0]\n",
    "        probs = F.softmax(logits - logits_max.detach(), dim=-1)   # calculate probs along patterns (NOT queries) ie. along topk --> [N, topk]\n",
    "\n",
    "        # weighted sum over topk_X: x_ij * probs_i\n",
    "        xi_new = torch.sum(probs.unsqueeze(-1) * topk_X, dim=1)\n",
    "\n",
    "        return xi_new\n",
    "    \n",
    "    def __has_converged(self, old_xi, new_xi): \n",
    "        \"\"\" \n",
    "        Checks whether or not the hopfield network has converged. Convergence is measured through taking the average cosine similarity \n",
    "        between old_xi and new_xi. If this average meets the threshold (ie. avg_cos_sim >= threshold), then we say that old_xi and \n",
    "        new_xi are the same and that the hopfield network has converged. \n",
    "\n",
    "        old_xi and new_xi are shapes: [N, d]\n",
    "\n",
    "        Args: \n",
    "            old_xi xi before running the udpate rule.\n",
    "            new_xi: xi after running the update rule.\n",
    "        \n",
    "        Returns:\n",
    "            True: if the average cosine similarity between old_xi and new_xi is meets the threshold.\n",
    "            False: if the average cosine similarity between old_xi and new_xi is below the threshold.\n",
    "        \"\"\"\n",
    "        converged = False \n",
    "\n",
    "        old_norm = F.normalize(old_xi, p=2, dim=-1)  # normalize along rows\n",
    "        new_norm = F.normalize(new_xi, p=2, dim=-1)\n",
    "\n",
    "        cos_sim = torch.sum(old_norm * new_norm, dim=1)  # [N], similarity for each query\n",
    "\n",
    "        min_cos_sim = cos_sim.min().item()\n",
    "        if min_cos_sim >= self.threshold:\n",
    "            converged = True\n",
    "\n",
    "        return converged\n",
    "\n",
    "    def run(self, X, xi, beta=None, run_as_batch=False): \n",
    "        \"\"\"\n",
    "        Runs the network. \n",
    "\n",
    "        Args: \n",
    "            X: The stored patterns. X is of size [N, d], where B is the batches, N is the number of patterns, and d the size of the patterns. \n",
    "            xi: The state pattern (ie. the current pattern being updated). xi is of size [d, 1]. xi can also be a batch of queries [N, d].\n",
    "            beta: The scalar inverse-temperature hyperparamater. Controls the number of metastable states that occur in the energy landscape. \n",
    "                - High beta corresponds to low temp, more separation between patterns.  \n",
    "                - Low beta corresponds to high temp, less separation (more metastable states). \n",
    "        \"\"\"\n",
    "        assert beta != None, \"Must have a value for beta.\"\n",
    "\n",
    "        if not isinstance(beta, torch.Tensor):\n",
    "           beta = torch.as_tensor(beta, dtype=torch.float32)\n",
    "\n",
    "        X = X.to(self.device)\n",
    "        xi = xi.to(self.device)\n",
    "        beta = beta.to(self.device)\n",
    "\n",
    "        if run_as_batch: \n",
    "            if xi.dim() == 1: \n",
    "                raise ValueError(\"Query shape should be [N, d] when updating as a batch.\")\n",
    "            for _ in range(self.max_iter): \n",
    "                old_xi = xi.clone()\n",
    "                xi = self.__run_batch(X, xi, beta)\n",
    "\n",
    "                if self.__has_converged(old_xi=old_xi, new_xi=xi): \n",
    "                    break \n",
    "            return xi\n",
    "\n",
    "        else:\n",
    "            # if xi is of size [d], then change to [d, 1] \n",
    "            if xi.dim() == 1: \n",
    "                xi = xi.unsqueeze(1) #[d, 1]\n",
    "            elif xi.dim() == 2 and xi.size(1) != 1: \n",
    "                raise ValueError(\"Query shape should be [d] or [d, 1].\") \n",
    "\n",
    "            for _ in range(self.max_iter): \n",
    "                xi = self.__update(X, xi, beta)\n",
    "            return xi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedBetaModel(pl.LightningModule): \n",
    "    def __init__(self, cmhn, beta_max=200, lr=1e-3, weight_decay=1e-5, temperature=1, masking_ratio=0.3, max_epochs=1000, input_dim=32, h1=128, h2=32, fc_h1 = 256, fc_h2 = 128, fc_h3 = 64, device=\"cpu\"):\n",
    "        super().__init__() \n",
    "        self.save_hyperparameters()\n",
    "        self.cmhn = cmhn \n",
    "        self.device_type = torch.device(device=device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=masking_ratio, inplace=False)\n",
    "\n",
    "        self.beta_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            nn.ReLU(), \n",
    "\n",
    "            nn.Linear(h1, h2), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(h2, 1),\n",
    "            nn.Softplus() \n",
    "            #nn.Sigmoid()       12/20/2025: removing sigmoid for softplus + clamping, because sigmoid is too steep and can cause beta to easily max out  \n",
    "        ).to(self.device_type)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(params=self.parameters(), \n",
    "                                lr= self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr / 50)\n",
    "        return ([optimizer], [lr_scheduler])\n",
    "\n",
    "    def loss(self, batch, mode=\"train\"): \n",
    "        \"\"\"\n",
    "        The loss function for the beta network. \n",
    "\n",
    "        Args: \n",
    "            batch: The batch data that the beta network will use (z representations). \n",
    "        \n",
    "        Returns: \n",
    "            loss: The infoNCE loss. \n",
    "        \"\"\"\n",
    "        batch_norm = F.normalize(batch, p=2, dim=-1)        \n",
    "        N, d = batch_norm.shape\n",
    "\n",
    "        # build the faiss index beforehand\n",
    "        self.cmhn.build_index(X=batch_norm, d=d)\n",
    "\n",
    "        # get the trial beta \n",
    "        beta = torch.mean(self.beta_net(batch_norm)) * self.hparams.beta_max\n",
    "        \n",
    "        beta = beta.clamp(max = self.hparams.beta_max)\n",
    "\n",
    "        # get abstract representation 'u' and normalize \n",
    "        #u = F.normalize(self.cmhn.run(batch_norm, batch_norm, beta, run_as_batch=True) , p=2, dim=-1)\n",
    "\n",
    "        # get the noisy batch, nn.Dropout uses scaling=True to maintain expected value of tensor\n",
    "        z_prime = F.normalize(self.dropout(batch), p=2, dim=-1) # adding normalization to dropout, since cos_sim doesn't care about vector magnitude \n",
    "\n",
    "        #get u' from z' and normalize\n",
    "        #u_prime = F.normalize(self.cmhn.run(batch_norm, z_prime, beta, run_as_batch=True), p=2, dim=-1)  # removing this as a test\n",
    "\n",
    "        # concat batch_norm and z_prime together to pass them both into one run of the cmhn (faster than two separate runs of cmhn and stronger backprop signal)\n",
    "        queries_combined = torch.cat([batch_norm, z_prime], dim=0)\n",
    "        u_combined = self.cmhn.run(batch_norm, queries_combined, beta, run_as_batch=True)\n",
    "        u = u_combined[:N]\n",
    "        u_prime = u_combined[N:]\n",
    "\n",
    "        # create positive pairs\n",
    "        p = torch.cat([u, u_prime], dim=0)\n",
    "\n",
    "        # put new batch pairs into fc_nn to obtain vectors in new embedding space useful for contrastive learning \n",
    "        ######################################################################\n",
    "        #     use p for contrastive loss \n",
    "        ######################################################################\n",
    "\n",
    "        N = p.size(0) // 2\n",
    "        p_norm = F.normalize(p, p=2, dim=-1)  \n",
    "        \n",
    "        sim = torch.matmul(p_norm, p_norm.T) / self.hparams.temperature # cosine sim matrix [2N, 2N]\n",
    "        if mode==\"train\": \n",
    "            with torch.no_grad():\n",
    "                self.log(f\"{mode}/sim_mean\", sim.mean(), on_epoch=True)\n",
    "                self.log(f\"{mode}/sim_std\", sim.std(), on_epoch=True)\n",
    "                sim_xy = torch.mean(torch.sum(batch_norm * u, dim=-1))\n",
    "                self.log(f\"{mode}/sim_xy\", sim_xy, on_epoch=True)\n",
    "\n",
    "        # mask diagonals to large negative numbers so we don't calculate same state similarities\n",
    "        mask = torch.eye(2 * N, device=sim.device).bool()\n",
    "        sim = sim.masked_fill_(mask, -9e15)\n",
    "\n",
    "        # positives: i-th sample matches i + N mod 2N\n",
    "        labels = (torch.arange(2 * N, device=sim.device) + N) % (2 * N)\n",
    "\n",
    "        loss = F.cross_entropy(sim, labels) # over mean reduction \n",
    "\n",
    "        # extra statistics \n",
    "        if mode==\"train\": \n",
    "            with torch.no_grad(): \n",
    "                self.log(f\"{mode}/p_norm_mean\", p_norm.mean(), on_epoch=True)\n",
    "                self.log(f\"{mode}/p_norm_std\", p_norm.std(), on_epoch=True)\n",
    "                self.log(f\"{mode}/beta\", beta.item(), on_epoch=True)\n",
    "\n",
    "                self.log(f\"{mode}/U_norm_mean\", u.mean(), on_epoch=True)\n",
    "                self.log(f\"{mode}/U_norm_std\", u.std(), on_epoch=True)\n",
    "                self.log(f\"{mode}/U_norm_max\", u.max(), on_epoch=True)\n",
    "\n",
    "        # metrics\n",
    "        preds = sim.argmax(dim=1)\n",
    "        top1 = (preds == labels).float().mean()   # top1: true positive is most similar to anchor \n",
    "        top5 = (sim.topk(5, dim=1).indices == labels.unsqueeze(1)).any(dim=1).float().mean() # top5: true positive is atleast in the top 5 most similar to anchor \n",
    "\n",
    "        self.log(f\"{mode}/nll_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top1\", top1, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{mode}/top5\", top5, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        return self.loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self.loss(batch, mode='val')\n",
    "\n",
    "    def get_beta(self, batch): \n",
    "        \"\"\"Returns the beta value.\"\"\"\n",
    "        beta = self.beta_net(F.normalize(batch, dim=-1))\n",
    "        beta = (torch.mean(beta) * self.hparams.beta_max).clamp(max=self.hparams.beta_max)\n",
    "        return beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cmhn model \n",
    "mhn = cmhn(max_iter=1, threshold=0, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_states(dataset=MINARI_DATASET, num_states=10)\n",
    "states = data[\"states\"]\n",
    "\n",
    "states = torch.as_tensor(states, dtype=torch.float32)\n",
    "with torch.no_grad(): \n",
    "    z = cl_model(states) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbm = LearnedBetaModel(cmhn=mhn, beta_max=200, device=DEVICE)\\n_, d = z.shape \\nz_norm = F.normalize(z, p=2, dim=-1)\\nmhn.build_index(X=z_norm, d=d)\\nloss = bm.loss(z)\\nloss \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bm = LearnedBetaModel(cmhn=mhn, beta_max=200, device=DEVICE)\n",
    "_, d = z.shape \n",
    "z_norm = F.normalize(z, p=2, dim=-1)\n",
    "mhn.build_index(X=z_norm, d=d)\n",
    "loss = bm.loss(z)\n",
    "loss \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.19704270362854\n",
      "beta_net 0.weight (0.00012130623508710414, 8.010457577256602e-07, 1.9972449081251398e-05)\n",
      "beta_net 0.bias (0.00012225241516716778, 6.079752893128898e-06, 4.129078661208041e-05)\n",
      "beta_net 2.weight (0.0003270024317316711, 1.9494175376166822e-06, 4.903965600533411e-05)\n",
      "beta_net 2.bias (0.00029898397042416036, 3.445614129304886e-05, 0.00011448246368672699)\n",
      "beta_net 4.weight (0.00019059520855080336, 2.0398505512275733e-05, 0.00010913576988968998)\n",
      "beta_net 4.bias (0.0006565195508301258, 0.0006565195508301258, 0.0006565195508301258)\n",
      "tensor(140.4356, device='cuda:0', grad_fn=<ClampBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_gpu\\lib\\site-packages\\pytorch_lightning\\core\\module.py:449: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "from src.data.StatesDataset import StatesDataset\n",
    "\n",
    "bm = LearnedBetaModel(cmhn=mhn, beta_max=200, device=DEVICE)\n",
    "\n",
    "train_ds = StatesDataset(cl_model=cl_model, minari_dataset=MINARI_DATASET, data=states)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=5, shuffle=True, drop_last=True)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "bm.train()\n",
    "batch = next(iter(train_loader)).to(DEVICE)\n",
    "\n",
    "loss = bm.loss(batch)\n",
    "bm.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "\n",
    "def grad_stat(p):\n",
    "    if p.grad is None:\n",
    "        return None\n",
    "    return (p.grad.norm().item(), p.grad.abs().mean().item(), p.grad.abs().max().item())\n",
    "\n",
    "print(\"loss:\", loss.item())\n",
    "\n",
    "# beta_net grads\n",
    "for name, p in bm.beta_net.named_parameters():\n",
    "    print(\"beta_net\", name, grad_stat(p))\n",
    "\n",
    "print(bm.get_beta(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_gpu\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | dropout  | Dropout    | 0      | train\n",
      "1 | beta_net | Sequential | 8.4 K  | train\n",
      "------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.4 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_gpu\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_gpu\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with profiler...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.31it/s, v_num=11, train/nll_loss_step=2.150, train/top1_step=0.500, train/top5_step=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ray\\AppData\\Local\\anaconda3\\envs\\CL_RL_gpu\\lib\\site-packages\\pytorch_lightning\\profilers\\pytorch.py:467: The PyTorch Profiler default schedule will be overridden as there is not enough steps to properly record traces.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.99it/s, v_num=11, train/nll_loss_step=2.170, train/top1_step=0.400, train/top5_step=0.700, train/nll_loss_epoch=2.160, train/top1_epoch=0.450, train/top5_epoch=0.850]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.99it/s, v_num=11, train/nll_loss_step=2.170, train/top1_step=0.400, train/top5_step=0.700, train/nll_loss_epoch=2.160, train/top1_epoch=0.450, train/top5_epoch=0.850]\n",
      "Profiling complete.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "\n",
    "from src.data.StatesDataset import StatesDataset\n",
    "\n",
    "# 1. Instantiate the profiler\n",
    "# We'll use the built-in PyTorch profiler. \n",
    "# We'll save the results to the 'lightning_logs' directory for TensorBoard.\n",
    "profiler = PyTorchProfiler(\n",
    "    dirpath=\"lightning_logs\",\n",
    "    filename=\"cmhn_profile\",\n",
    "    # We only need a few steps to check the gradients\n",
    ")\n",
    "\n",
    "# 2. Instantiate the Trainer\n",
    "# Set a limit on the number of batches to run so the profiler finishes quickly.\n",
    "trainer = pl.Trainer(\n",
    "    profiler=profiler,\n",
    "    limit_train_batches=10,  # Run just enough batches to hit the profiling schedule\n",
    "    max_epochs=1,\n",
    "    # Add other necessary args like devices=1, accelerator='cuda' if using GPU\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = StatesDataset(cl_model=cl_model, minari_dataset=MINARI_DATASET, data=states)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=5, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "# 3. Run training\n",
    "print(\"Starting training with profiler...\")\n",
    "trainer.fit(bm, train_loader)\n",
    "print(\"Profiling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CL_RL_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
